{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDExvtEuPa_6",
        "outputId": "44679093-dd8b-4733-da52-2d6df25d03c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "<ipython-input-1-2d867d116d23>:29: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['text'] = train_data['text'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-1-2d867d116d23>:30: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  train_data['text'] = train_data['text'].str.replace('\\d+', '')\n",
            "<ipython-input-1-2d867d116d23>:34: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  val_data['text'] = val_data['text'].str.replace('[^\\w\\s]','')\n",
            "<ipython-input-1-2d867d116d23>:35: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  val_data['text'] = val_data['text'].str.replace('\\d+', '')\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 1, 'max_iter': 1000}\n",
            "Test accuracy: 0.99025\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/content/Train.csv')\n",
        "val_data = pd.read_csv('/content/Valid.csv')\n",
        "test_data = pd.read_csv('/content/Test.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_data['text'] = train_data['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = set(['list', 'of', 'custom', 'stopwords'])\n",
        "stopwords = ENGLISH_STOP_WORDS.union(custom_stopwords)\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
        "\n",
        "# Data preparation\n",
        "train_data['text'] = train_data['text'].str.lower()\n",
        "train_data['text'] = train_data['text'].str.replace('[^\\w\\s]','')\n",
        "train_data['text'] = train_data['text'].str.replace('\\d+', '')\n",
        "train_data['text'] = train_data['text'].str.strip()\n",
        "\n",
        "val_data['text'] = val_data['text'].str.lower()\n",
        "val_data['text'] = val_data['text'].str.replace('[^\\w\\s]','')\n",
        "val_data['text'] = val_data['text'].str.replace('\\d+', '')\n",
        "val_data['text'] = val_data['text'].str.strip()\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_data['text'], train_data['label'], test_size=0.2, random_state=42)\n",
        "X_val = val_data['text']\n",
        "y_val = val_data['label']\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Hyperparameter tuning using grid search\n",
        "C_values = [0.1, 1, 10, 100]\n",
        "max_iter_values = [1000, 10000, 100000]\n",
        "param_grid = {'C': C_values, 'max_iter': max_iter_values}\n",
        "svm = LinearSVC()\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_max_iter = grid_search.best_params_['max_iter']\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "\n",
        "# Train the final model with best hyperparameters\n",
        "svm = LinearSVC(C=best_C, max_iter=best_max_iter)\n",
        "X_full = pd.concat([train_data['text'], val_data['text']], axis=0)\n",
        "y_full = pd.concat([train_data['label'], val_data['label']], axis=0)\n",
        "X_full_tfidf = tfidf_vectorizer.fit_transform(X_full)\n",
        "svm.fit(X_full_tfidf, y_full)\n",
        "\n",
        "# Model evaluation on test set\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "y_pred_test = svm.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDtOH2qNPbgW",
        "outputId": "8ef593da-d91d-4407-b789-ef84311b4a51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 0.1, 'max_iter': 1000}\n",
            "Test accuracy: 0.937\n"
          ]
        }
      ],
      "source": [
        "# Additional steps to improve accuracy\n",
        "# 1. Increase the size of the training data\n",
        "X_train_full = pd.concat([train_data['text'], val_data['text']], axis=0)\n",
        "y_train_full = pd.concat([train_data['label'], val_data['label']], axis=0)\n",
        "X_train_full_tfidf = tfidf_vectorizer.transform(X_train_full)\n",
        "svm.fit(X_train_full_tfidf, y_train_full)\n",
        "\n",
        "# 2. Try different values for C and max_iter\n",
        "C_values = [0.01, 0.1, 1, 10, 100]\n",
        "max_iter_values = [1000, 10000, 100000]\n",
        "param_grid = {'C': C_values, 'max_iter': max_iter_values}\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X_train_full_tfidf, y_train_full)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_max_iter = grid_search.best_params_['max_iter']\n",
        "print('Best hyperparameters:', grid_search.best_params_)\n",
        "\n",
        "# Train the final model with the best hyperparameters\n",
        "svm = LinearSVC(C=best_C, max_iter=best_max_iter)\n",
        "svm.fit(X_train_full_tfidf, y_train_full)\n",
        "\n",
        "# Model evaluation on test set\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "y_pred_test = svm.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred_test)\n",
        "print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# "
      ],
      "metadata": {
        "id": "HJuMhaj2wTzN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create and fit the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_full)\n",
        "\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "with open('model.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)\n",
        "\n",
        "\n",
        "\n",
        "with open('X_tra_tfidf.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)\n",
        "\n",
        "with open('X_tra_tfidf.pkl', 'rb') as file:\n",
        "    tfidf_vectorizer = pickle.load(file)\n",
        "\n",
        "# Perform transformation on the test data using the loaded vectorizer\n",
        "x = tfidf_vectorizer.transform([X_val[127]])\n",
        "# print(x)\n",
        "# Make predictions on the transformed test data using the loaded model\n",
        "y_pred_test = model.predict(x)"
      ],
      "metadata": {
        "id": "puVpsXE-5S44",
        "outputId": "20b5393b-d417-4c09-e4b1-c69f17e58b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 163557)\t0.1023711757343013\n",
            "  (0, 157937)\t0.1273416287832618\n",
            "  (0, 147609)\t0.10792221141523038\n",
            "  (0, 146809)\t0.05066695123922059\n",
            "  (0, 145727)\t0.06286042017270566\n",
            "  (0, 145024)\t0.15609797826242466\n",
            "  (0, 142876)\t0.10936512973880876\n",
            "  (0, 140768)\t0.22983559041771545\n",
            "  (0, 140675)\t0.14465634060838314\n",
            "  (0, 136865)\t0.15556789538441032\n",
            "  (0, 136225)\t0.20424907960385158\n",
            "  (0, 131140)\t0.12262118122234779\n",
            "  (0, 129010)\t0.10973514025156242\n",
            "  (0, 126865)\t0.0895271288715064\n",
            "  (0, 126316)\t0.10079004069083665\n",
            "  (0, 124366)\t0.0901807788928747\n",
            "  (0, 122566)\t0.08070483270639874\n",
            "  (0, 119694)\t0.13785636278511637\n",
            "  (0, 116117)\t0.11060911237466318\n",
            "  (0, 111563)\t0.12515380965373774\n",
            "  (0, 110865)\t0.07999547828422703\n",
            "  (0, 110304)\t0.13907700982115617\n",
            "  (0, 108152)\t0.11758524701332583\n",
            "  (0, 107972)\t0.0993453918230032\n",
            "  (0, 106811)\t0.10529063340383536\n",
            "  :\t:\n",
            "  (0, 82836)\t0.0445446515999761\n",
            "  (0, 78181)\t0.08415043022211138\n",
            "  (0, 74507)\t0.11344855149962992\n",
            "  (0, 74172)\t0.08062841024965584\n",
            "  (0, 70051)\t0.07317394226276744\n",
            "  (0, 69534)\t0.1020707650485787\n",
            "  (0, 67082)\t0.19702249381620718\n",
            "  (0, 65450)\t0.1771061173806621\n",
            "  (0, 61556)\t0.12104738634136827\n",
            "  (0, 59247)\t0.05004501551506329\n",
            "  (0, 55593)\t0.07772514933199486\n",
            "  (0, 54302)\t0.14821602234375159\n",
            "  (0, 52427)\t0.1376093593971794\n",
            "  (0, 52352)\t0.17031303318615595\n",
            "  (0, 47336)\t0.13084165655511704\n",
            "  (0, 47177)\t0.14952425276323142\n",
            "  (0, 27828)\t0.08460518420614306\n",
            "  (0, 23578)\t0.05721946005006808\n",
            "  (0, 17311)\t0.15439999729198167\n",
            "  (0, 16193)\t0.14241643808175086\n",
            "  (0, 14985)\t0.16709462575563044\n",
            "  (0, 8767)\t0.16067582796787525\n",
            "  (0, 8194)\t0.1434493301780186\n",
            "  (0, 1400)\t0.07144991412882375\n",
            "  (0, 924)\t0.13960365473201355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test"
      ],
      "metadata": {
        "id": "4Mpxyi9j9hNc",
        "outputId": "885342c5-e058-4d78-c369-da81dbffac1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[X_val[2]]"
      ],
      "metadata": {
        "id": "MzkfZxPCFeKD",
        "outputId": "43a1012e-514f-423b-cd90-82e25ebba470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the guidelines state that a comment must contain a minimum of four lines that is the only reason i am saying anything more about tomcats because after all my one line summary really says everything there is to say there is absolutely nothing remotely entertaining in this film']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tfidf_vectorizer.transform([X_val[2]])\n",
        "print(x)\n",
        "# Make predictions on the transformed test data using the loaded model\n",
        "y_pred_test = model.predict(x)\n",
        "y_pred_test\n"
      ],
      "metadata": {
        "id": "xeOImkhG57lQ",
        "outputId": "4521a41c-1e0d-4907-8971-96a3c6849837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 147713)\t0.42832562697341464\n",
            "  (0, 140704)\t0.2600426417605123\n",
            "  (0, 137684)\t0.2118262890256179\n",
            "  (0, 125599)\t0.24195033192660448\n",
            "  (0, 125565)\t0.19026238631496886\n",
            "  (0, 125542)\t0.11589472679246648\n",
            "  (0, 119740)\t0.2600426417605123\n",
            "  (0, 118025)\t0.1508725963141069\n",
            "  (0, 117915)\t0.09667866813120438\n",
            "  (0, 92091)\t0.2951302263048865\n",
            "  (0, 83150)\t0.21661613309114133\n",
            "  (0, 83108)\t0.14957184603430104\n",
            "  (0, 61383)\t0.4126863218308397\n",
            "  (0, 51222)\t0.06925815059992299\n",
            "  (0, 45517)\t0.1746615607725019\n",
            "  (0, 29612)\t0.2656478157106921\n",
            "  (0, 28078)\t0.18530596396472493\n",
            "  (0, 582)\t0.17095393057334457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your trained model object is named 'model'\n",
        "with open('X_train_tfidf.pkl', 'wb') as file:\n",
        "    pickle.dump(X_train_tfidf, file)"
      ],
      "metadata": {
        "id": "mvcBBbP1xE-V"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('X_train_tfidf.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)"
      ],
      "metadata": {
        "id": "h0lJrIZYxFA8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6vyyT_4_Pbiz"
      },
      "outputs": [],
      "source": [
        "# Generate predictions on the competition test set\n",
        "X_comp_test_tfidf = tfidf_vectorizer.transform(test_data['text'])\n",
        "comp_test_predictions = svm.predict(X_comp_test_tfidf)\n",
        "\n",
        "# Prepare submission file\n",
        "submission_df = pd.DataFrame({'Id': test_data.iloc[:, 0], 'Label': comp_test_predictions})\n",
        "\n",
        "# Save submission file\n",
        "submission_df.to_csv('/content/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4rHlitdePbln"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZT7IsKKuhX5R"
      },
      "outputs": [],
      "source": [
        "# Assuming your trained model object is named 'model'\n",
        "with open('model.pkl', 'wb') as file:\n",
        "    pickle.dump(svm, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)"
      ],
      "metadata": {
        "id": "p2iDAbmqevRL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "345eV0ZAevUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-zLqNht4hX-v"
      },
      "outputs": [],
      "source": [
        "# Assuming your trained model object is named 'model'\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
        "    pickle.dump(tfidf_vectorizer, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yy6q5IybhYDO"
      },
      "outputs": [],
      "source": [
        "with open('tfidf_vectorizer.pkl', 'rb') as file:\n",
        "    model = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL1VfyxyhYGZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}